{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70d5a1f2",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-31T17:34:46.813195Z",
     "iopub.status.busy": "2025-01-31T17:34:46.812843Z",
     "iopub.status.idle": "2025-01-31T17:34:47.773234Z",
     "shell.execute_reply": "2025-01-31T17:34:47.772258Z"
    },
    "papermill": {
     "duration": 0.973212,
     "end_time": "2025-01-31T17:34:47.775201",
     "exception": false,
     "start_time": "2025-01-31T17:34:46.801989",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68496767",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T14:19:29.983560Z",
     "iopub.status.busy": "2025-01-31T14:19:29.983237Z",
     "iopub.status.idle": "2025-01-31T14:19:53.301107Z",
     "shell.execute_reply": "2025-01-31T14:19:53.300269Z",
     "shell.execute_reply.started": "2025-01-31T14:19:29.983532Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2025-01-31T17:34:47.782989",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing dependencies of bleach: Expected matching RIGHT_PARENTHESIS for LEFT_PARENTHESIS, after version specifier\n",
      "    tinycss2 (>=1.1.0<1.2) ; extra == 'css'\n",
      "             ~~~~~~~~^\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aeon 1.0.0 requires scipy<1.15.0,>=1.9.0, but you have scipy 1.15.2 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -U bitsandbytes transformers peft accelerate datasets scipy einops evaluate trl rouge_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2239d8",
   "metadata": {},
   "source": [
    "Rouge_score didn't have property __version__ so removed it from print."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2120e00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T14:21:19.774492Z",
     "iopub.status.busy": "2025-01-31T14:21:19.774181Z",
     "iopub.status.idle": "2025-01-31T14:21:40.216085Z",
     "shell.execute_reply": "2025-01-31T14:21:40.214716Z",
     "shell.execute_reply.started": "2025-01-31T14:21:19.774469Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BitsAndBytes version: 0.45.2\n",
      "Transformers version: 4.49.0\n",
      "PEFT version: 0.14.0\n",
      "Accelerate version: 1.4.0\n",
      "Datasets version: 3.3.1\n",
      "Scipy version: 1.15.2\n",
      "Einops version: 0.8.1\n",
      "Evaluate version: 0.4.3\n",
      "TRL version: 0.15.1\n"
     ]
    }
   ],
   "source": [
    "import bitsandbytes\n",
    "import transformers\n",
    "import peft\n",
    "import accelerate\n",
    "import datasets\n",
    "import scipy\n",
    "import einops\n",
    "import evaluate\n",
    "import trl\n",
    "import rouge_score\n",
    "\n",
    "print(\"BitsAndBytes version:\", bitsandbytes.__version__)\n",
    "print(\"Transformers version:\", transformers.__version__)\n",
    "print(\"PEFT version:\", peft.__version__)\n",
    "print(\"Accelerate version:\", accelerate.__version__)\n",
    "print(\"Datasets version:\", datasets.__version__)\n",
    "print(\"Scipy version:\", scipy.__version__)\n",
    "print(\"Einops version:\", einops.__version__)\n",
    "print(\"Evaluate version:\", evaluate.__version__)\n",
    "print(\"TRL version:\", trl.__version__)\n",
    "#print(\"Rouge Score version:\", rouge_score.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcc98613",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T14:22:31.448443Z",
     "iopub.status.busy": "2025-01-31T14:22:31.448165Z",
     "iopub.status.idle": "2025-01-31T14:22:31.452192Z",
     "shell.execute_reply": "2025-01-31T14:22:31.451233Z",
     "shell.execute_reply.started": "2025-01-31T14:22:31.448423Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# disable Weights and Biases\n",
    "os.environ['WANDB_DISABLED']=\"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2c2d24",
   "metadata": {},
   "source": [
    "Had an issue with interpreter_login showing up, so used login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bcb0c19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T14:23:21.830631Z",
     "iopub.status.busy": "2025-01-31T14:23:21.830321Z",
     "iopub.status.idle": "2025-01-31T14:23:25.961978Z",
     "shell.execute_reply": "2025-01-31T14:23:25.961287Z",
     "shell.execute_reply.started": "2025-01-31T14:23:21.830609Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa0d9e0b33c4bcdbd49994ad8c75893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    GenerationConfig\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from trl import SFTTrainer\n",
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdc28769",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T14:23:45.482772Z",
     "iopub.status.busy": "2025-01-31T14:23:45.482466Z",
     "iopub.status.idle": "2025-01-31T14:23:45.486970Z",
     "shell.execute_reply": "2025-01-31T14:23:45.486026Z",
     "shell.execute_reply.started": "2025-01-31T14:23:45.482747Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pynvml import *\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bd9f448",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T14:23:58.192663Z",
     "iopub.status.busy": "2025-01-31T14:23:58.192374Z",
     "iopub.status.idle": "2025-01-31T14:24:01.771245Z",
     "shell.execute_reply": "2025-01-31T14:24:01.770618Z",
     "shell.execute_reply.started": "2025-01-31T14:23:58.192642Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 1999\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 499\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 499\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://huggingface.co/datasets/neil-code/dialogsum-test\n",
    "huggingface_dataset_name = \"neil-code/dialogsum-test\"\n",
    "dataset = load_dataset(huggingface_dataset_name)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "484c65f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T14:24:18.727693Z",
     "iopub.status.busy": "2025-01-31T14:24:18.727411Z",
     "iopub.status.idle": "2025-01-31T14:24:18.734332Z",
     "shell.execute_reply": "2025-01-31T14:24:18.733589Z",
     "shell.execute_reply.started": "2025-01-31T14:24:18.727672Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'train_0',\n",
       " 'dialogue': \"#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\\n#Person2#: I found it would be a good idea to get a check-up.\\n#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\\n#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\\n#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\\n#Person2#: Ok.\\n#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\\n#Person2#: Yes.\\n#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\\n#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\\n#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\\n#Person2#: Ok, thanks doctor.\",\n",
       " 'summary': \"Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\",\n",
       " 'topic': 'get a check-up'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ece06e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T14:24:43.136185Z",
     "iopub.status.busy": "2025-01-31T14:24:43.135849Z",
     "iopub.status.idle": "2025-01-31T14:24:43.142066Z",
     "shell.execute_reply": "2025-01-31T14:24:43.141204Z",
     "shell.execute_reply.started": "2025-01-31T14:24:43.136161Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch, \"float16\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "    )\n",
    "device_map = {\"\": 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae0e4a0",
   "metadata": {},
   "source": [
    "Changed model to TinyLlama-1.1B-Chat-V1.0. First tried to use base model for the task, but even after fine-tuning it didn't work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "877a49ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T14:24:53.973914Z",
     "iopub.status.busy": "2025-01-31T14:24:53.973626Z",
     "iopub.status.idle": "2025-01-31T14:27:10.924279Z",
     "shell.execute_reply": "2025-01-31T14:27:10.923619Z",
     "shell.execute_reply.started": "2025-01-31T14:24:53.973894Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\roope\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name='TinyLlama/TinyLlama-1.1B-Chat-v1.0'\n",
    "original_model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                                      device_map=device_map,\n",
    "                                                      quantization_config=bnb_config,\n",
    "                                                      trust_remote_code=True,\n",
    "                                                      use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e1b6f56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T14:28:12.022831Z",
     "iopub.status.busy": "2025-01-31T14:28:12.022526Z",
     "iopub.status.idle": "2025-01-31T14:28:14.548898Z",
     "shell.execute_reply": "2025-01-31T14:28:14.548123Z",
     "shell.execute_reply.started": "2025-01-31T14:28:12.022810Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://ai.stackexchange.com/questions/41485/while-fine-tuning-a-decoder-only-llm-like-llama-on-chat-dataset-what-kind-of-pa\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,trust_remote_code=True,padding_side=\"left\",add_eos_token=True,add_bos_token=True,use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3675e512",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T14:28:25.297983Z",
     "iopub.status.busy": "2025-01-31T14:28:25.297654Z",
     "iopub.status.idle": "2025-01-31T14:28:25.303177Z",
     "shell.execute_reply": "2025-01-31T14:28:25.302162Z",
     "shell.execute_reply.started": "2025-01-31T14:28:25.297922Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 1614 MB.\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c962ff14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T14:28:44.289558Z",
     "iopub.status.busy": "2025-01-31T14:28:44.289206Z",
     "iopub.status.idle": "2025-01-31T14:28:44.479182Z",
     "shell.execute_reply": "2025-01-31T14:28:44.478344Z",
     "shell.execute_reply.started": "2025-01-31T14:28:44.289531Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_tokenizer = AutoTokenizer.from_pretrained(model_name, add_bos_token=True, trust_remote_code=True, use_fast=False)\n",
    "eval_tokenizer.pad_token = eval_tokenizer.eos_token\n",
    "\n",
    "def gen(model,p, maxlen=100, sample=True):\n",
    "    toks = eval_tokenizer(p, return_tensors=\"pt\")\n",
    "    res = model.generate(**toks.to(\"cuda\"), max_new_tokens=maxlen, do_sample=sample,num_return_sequences=1,temperature=0.1,num_beams=1,top_p=0.95,).to('cpu')\n",
    "    return eval_tokenizer.batch_decode(res,skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33b5b4a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T14:28:59.715828Z",
     "iopub.status.busy": "2025-01-31T14:28:59.715496Z",
     "iopub.status.idle": "2025-01-31T14:29:03.062327Z",
     "shell.execute_reply": "2025-01-31T14:29:03.061453Z",
     "shell.execute_reply.started": "2025-01-31T14:28:59.715802Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "Instruct: Summarize the following conversation.\n",
      "#Person1#: Happy Birthday, this is for you, Brian.\n",
      "#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n",
      "#Person1#: Brian, may I have a pleasure to have a dance with you?\n",
      "#Person2#: Ok.\n",
      "#Person1#: This is really wonderful party.\n",
      "#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n",
      "#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n",
      "#Person2#: You look great, you are absolutely glowing.\n",
      "#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\n",
      "Output:\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "Brian: Happy Birthday, this is for you, Brian.\n",
      "Person2: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n",
      "Person1: Brian, may I have a pleasure to have a dance with you?\n",
      "Person2: Ok.\n",
      "Person1: This is really wonderful party.\n",
      "Person2: Yes, you are always popular with everyone. and you\n",
      "CPU times: total: 5.08 s\n",
      "Wall time: 6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import set_seed\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "index = 10\n",
    "\n",
    "prompt = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "\n",
    "formatted_prompt = f\"Instruct: Summarize the following conversation.\\n{prompt}\\nOutput:\\n\"\n",
    "res = gen(original_model,formatted_prompt,100,)\n",
    "#print(res[0])\n",
    "output = res[0].split('Output:\\n')[1]\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{formatted_prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c154282f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T14:29:55.868446Z",
     "iopub.status.busy": "2025-01-31T14:29:55.868027Z",
     "iopub.status.idle": "2025-01-31T14:29:55.874161Z",
     "shell.execute_reply": "2025-01-31T14:29:55.873267Z",
     "shell.execute_reply.started": "2025-01-31T14:29:55.868414Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_prompt_formats(sample):\n",
    "    \"\"\"\n",
    "    Format various fields of the sample ('instruction','output')\n",
    "    Then concatenate them using two newline characters \n",
    "    :param sample: Sample dictionnary\n",
    "    \"\"\"\n",
    "    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "    INSTRUCTION_KEY = \"### Instruct: Summarize the below conversation.\"\n",
    "    RESPONSE_KEY = \"### Output:\"\n",
    "    END_KEY = \"### End\"\n",
    "    \n",
    "    blurb = f\"\\n{INTRO_BLURB}\"\n",
    "    instruction = f\"{INSTRUCTION_KEY}\"\n",
    "    input_context = f\"{sample['dialogue']}\" if sample[\"dialogue\"] else None\n",
    "    response = f\"{RESPONSE_KEY}\\n{sample['summary']}\"\n",
    "    end = f\"{END_KEY}\"\n",
    "    \n",
    "    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n",
    "\n",
    "    formatted_prompt = \"\\n\\n\".join(parts)\n",
    "    sample[\"text\"] = formatted_prompt\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18955d00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T14:30:11.226598Z",
     "iopub.status.busy": "2025-01-31T14:30:11.226303Z",
     "iopub.status.idle": "2025-01-31T14:30:11.232046Z",
     "shell.execute_reply": "2025-01-31T14:30:11.231102Z",
     "shell.execute_reply.started": "2025-01-31T14:30:11.226577Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def get_max_length(model):\n",
    "    conf = model.config\n",
    "    max_length = None\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max lenth: {max_length}\")\n",
    "            break\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "    return max_length\n",
    "\n",
    "\n",
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Tokenizing a batch\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "609c7d52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T14:30:30.414730Z",
     "iopub.status.busy": "2025-01-31T14:30:30.414445Z",
     "iopub.status.idle": "2025-01-31T14:30:30.419851Z",
     "shell.execute_reply": "2025-01-31T14:30:30.418978Z",
     "shell.execute_reply.started": "2025-01-31T14:30:30.414708Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\n",
    "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int,seed, dataset):\n",
    "    \"\"\"Format & tokenize it so it is ready for training\n",
    "    :param tokenizer (AutoTokenizer): Model Tokenizer\n",
    "    :param max_length (int): Maximum number of tokens to emit from tokenizer\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add prompt to each sample\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    dataset = dataset.map(create_prompt_formats)#, batched=True)\n",
    "    \n",
    "    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched=True,\n",
    "        remove_columns=['id', 'topic', 'dialogue', 'summary'],\n",
    "    )\n",
    "\n",
    "    # Filter out samples that have input_ids exceeding max_length\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
    "    \n",
    "    # Shuffle dataset\n",
    "    dataset = dataset.shuffle(seed=seed)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20e0af1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T14:30:38.763207Z",
     "iopub.status.busy": "2025-01-31T14:30:38.762867Z",
     "iopub.status.idle": "2025-01-31T14:30:38.767618Z",
     "shell.execute_reply": "2025-01-31T14:30:38.766874Z",
     "shell.execute_reply.started": "2025-01-31T14:30:38.763182Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 1691 MB.\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25987aa1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T14:31:00.146357Z",
     "iopub.status.busy": "2025-01-31T14:31:00.145922Z",
     "iopub.status.idle": "2025-01-31T14:31:08.380427Z",
     "shell.execute_reply": "2025-01-31T14:31:08.379725Z",
     "shell.execute_reply.started": "2025-01-31T14:31:00.146324Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found max lenth: 2048\n",
      "2048\n",
      "Preprocessing dataset...\n",
      "Preprocessing dataset...\n",
      "Shapes of the datasets:\n",
      "Training: (1999, 3)\n",
      "Validation: (499, 3)\n",
      "Dataset({\n",
      "    features: ['text', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 1999\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# ## Pre-process dataset\n",
    "max_length = get_max_length(original_model)\n",
    "print(max_length)\n",
    "\n",
    "train_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['train'])\n",
    "eval_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['validation'])\n",
    "\n",
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {train_dataset.shape}\")\n",
    "print(f\"Validation: {eval_dataset.shape}\")\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf425cb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T14:32:05.057386Z",
     "iopub.status.busy": "2025-01-31T14:32:05.057057Z",
     "iopub.status.idle": "2025-01-31T14:32:05.064333Z",
     "shell.execute_reply": "2025-01-31T14:32:05.063419Z",
     "shell.execute_reply.started": "2025-01-31T14:32:05.057362Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 131164160\n",
      "all model parameters: 615606272\n",
      "percentage of trainable model parameters: 21.31%\n"
     ]
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "199ce7b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T14:32:36.476480Z",
     "iopub.status.busy": "2025-01-31T14:32:36.476182Z",
     "iopub.status.idle": "2025-01-31T14:32:36.482392Z",
     "shell.execute_reply": "2025-01-31T14:32:36.481581Z",
     "shell.execute_reply.started": "2025-01-31T14:32:36.476459Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-21): 22 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=5632, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(original_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41e56552",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T14:33:41.670614Z",
     "iopub.status.busy": "2025-01-31T14:33:41.670286Z",
     "iopub.status.idle": "2025-01-31T14:33:42.033227Z",
     "shell.execute_reply": "2025-01-31T14:33:42.032492Z",
     "shell.execute_reply.started": "2025-01-31T14:33:41.670593Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32, #Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        'q_proj',\n",
    "        'k_proj',\n",
    "        'v_proj',\n",
    "        'dense'\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\n",
    "original_model.gradient_checkpointing_enable()\n",
    "\n",
    "# 2 - Using the prepare_model_for_kbit_training method from PEFT\n",
    "original_model = prepare_model_for_kbit_training(original_model)\n",
    "\n",
    "peft_model = get_peft_model(original_model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a515d0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T14:34:00.457350Z",
     "iopub.status.busy": "2025-01-31T14:34:00.457042Z",
     "iopub.status.idle": "2025-01-31T14:34:00.466020Z",
     "shell.execute_reply": "2025-01-31T14:34:00.465013Z",
     "shell.execute_reply.started": "2025-01-31T14:34:00.457328Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 6127616\n",
      "all model parameters: 621733888\n",
      "percentage of trainable model parameters: 0.99%\n"
     ]
    }
   ],
   "source": [
    "print(print_number_of_trainable_model_parameters(peft_model))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c445a3c",
   "metadata": {},
   "source": [
    "Changes max_steps to 500, logging_steps to 50, and eval_steps to 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "96e12b4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T14:59:09.194579Z",
     "iopub.status.busy": "2025-01-31T14:59:09.194253Z",
     "iopub.status.idle": "2025-01-31T14:59:09.231274Z",
     "shell.execute_reply": "2025-01-31T14:59:09.230408Z",
     "shell.execute_reply.started": "2025-01-31T14:59:09.194552Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\roope\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-18 23:09:40,682] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-18 23:09:42,037] torch.distributed.elastic.multiprocessing.redirects: [WARNING] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "output_dir = './peft-dialogue-summary-training/final-checkpoint'\n",
    "import transformers\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    warmup_steps=1,\n",
    "    per_device_train_batch_size=4,  # âœ… Increased for better GPU usage\n",
    "    gradient_accumulation_steps=2,  # âœ… Reduced for speed\n",
    "    max_steps=500,\n",
    "    learning_rate=2e-4,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    logging_steps=50,  # âœ… Logs less often\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,  # âœ… Saves less often\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,  # âœ… Evaluates less often\n",
    "    do_eval=True,\n",
    "    gradient_checkpointing=False,  # âœ… Disabled for speed\n",
    "    fp16=True,  # âœ… Enables mixed precision\n",
    "    report_to=\"none\",\n",
    "    overwrite_output_dir=True,\n",
    "    group_by_length=True,\n",
    ")\n",
    "\n",
    "\n",
    "peft_model.config.use_cache = False\n",
    "\n",
    "peft_trainer = transformers.Trainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    args=peft_training_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6c2daa8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T14:59:12.791780Z",
     "iopub.status.busy": "2025-01-31T14:59:12.791486Z",
     "iopub.status.idle": "2025-01-31T17:19:21.196081Z",
     "shell.execute_reply": "2025-01-31T17:19:21.195203Z",
     "shell.execute_reply.started": "2025-01-31T14:59:12.791759Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\roope\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 18:25, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.433700</td>\n",
       "      <td>1.333315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.256800</td>\n",
       "      <td>1.312172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.288000</td>\n",
       "      <td>1.282949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.267800</td>\n",
       "      <td>1.273427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.275500</td>\n",
       "      <td>1.272381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.237700</td>\n",
       "      <td>1.266219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.250600</td>\n",
       "      <td>1.262309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.233600</td>\n",
       "      <td>1.260854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.224300</td>\n",
       "      <td>1.260180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.232500</td>\n",
       "      <td>1.259214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\roope\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\roope\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\roope\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "c:\\Users\\roope\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=1.2700594787597657, metrics={'train_runtime': 1110.008, 'train_samples_per_second': 3.604, 'train_steps_per_second': 0.45, 'total_flos': 7998465185280000.0, 'train_loss': 1.2700594787597657, 'epoch': 2.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_training_args.device\n",
    "peft_trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d158711e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T17:20:34.006639Z",
     "iopub.status.busy": "2025-01-31T17:20:34.006355Z",
     "iopub.status.idle": "2025-01-31T17:20:43.578312Z",
     "shell.execute_reply": "2025-01-31T17:20:43.577432Z",
     "shell.execute_reply.started": "2025-01-31T17:20:34.006619Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2903a84320bf46b1838701f30855c650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06da96baf5364c7ba2cb37c631ba0df4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\roope\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\roope\\.cache\\huggingface\\hub\\models--RoopeK--TinyLlama-dialog-finetuned. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cad026177aa4965bf409e39c58de323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/24.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88340bdb5d0c4cff8f27cb803fdc9b3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/RoopeK/TinyLlama-dialog-finetuned/commit/37ee3e83a87d252febd11cf5782910b72f65f63c', commit_message='Upload tokenizer', commit_description='', oid='37ee3e83a87d252febd11cf5782910b72f65f63c', pr_url=None, repo_url=RepoUrl('https://huggingface.co/RoopeK/TinyLlama-dialog-finetuned', endpoint='https://huggingface.co', repo_type='model', repo_id='RoopeK/TinyLlama-dialog-finetuned'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()  # Enter your HF token\n",
    "\n",
    "# Upload Model & Tokenizer\n",
    "peft_model.push_to_hub(\"RoopeK/TinyLlama-dialog-finetuned\")\n",
    "tokenizer.push_to_hub(\"RoopeK/TinyLlama-dialog-finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "83311711",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T17:22:52.932100Z",
     "iopub.status.busy": "2025-01-31T17:22:52.931780Z",
     "iopub.status.idle": "2025-01-31T17:22:52.958054Z",
     "shell.execute_reply": "2025-01-31T17:22:52.956888Z",
     "shell.execute_reply.started": "2025-01-31T17:22:52.932077Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 7980 MB.\n",
      "GPU memory occupied: 1866 MB.\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()\n",
    "del original_model\n",
    "del peft_trainer\n",
    "torch.cuda.empty_cache()\n",
    "print_gpu_utilization()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7077bce4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T17:23:21.573664Z",
     "iopub.status.busy": "2025-01-31T17:23:21.573358Z",
     "iopub.status.idle": "2025-01-31T17:23:25.138456Z",
     "shell.execute_reply": "2025-01-31T17:23:25.137808Z",
     "shell.execute_reply.started": "2025-01-31T17:23:21.573642Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\roope\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "base_model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_id, \n",
    "                                                      device_map='auto',\n",
    "                                                      quantization_config=bnb_config,\n",
    "                                                      trust_remote_code=True,\n",
    "                                                      use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6196f34d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T17:23:43.344832Z",
     "iopub.status.busy": "2025-01-31T17:23:43.344546Z",
     "iopub.status.idle": "2025-01-31T17:23:43.555778Z",
     "shell.execute_reply": "2025-01-31T17:23:43.555124Z",
     "shell.execute_reply.started": "2025-01-31T17:23:43.344810Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True, use_fast=False)\n",
    "eval_tokenizer.pad_token = eval_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f05c79c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T17:24:14.583982Z",
     "iopub.status.busy": "2025-01-31T17:24:14.583681Z",
     "iopub.status.idle": "2025-01-31T17:24:15.072784Z",
     "shell.execute_reply": "2025-01-31T17:24:15.072132Z",
     "shell.execute_reply.started": "2025-01-31T17:24:14.583959Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "ft_model = PeftModel.from_pretrained(base_model, \"peft-dialogue-summary-training/final-checkpoint/checkpoint-500\",torch_dtype=torch.float16,is_trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f9b49ae3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T17:24:30.180091Z",
     "iopub.status.busy": "2025-01-31T17:24:30.179789Z",
     "iopub.status.idle": "2025-01-31T17:24:36.889722Z",
     "shell.execute_reply": "2025-01-31T17:24:36.888817Z",
     "shell.execute_reply.started": "2025-01-31T17:24:30.180068Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "Instruct: Summarize the following conversation.\n",
      "#Person1#: Happy Birthday, this is for you, Brian.\n",
      "#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n",
      "#Person1#: Brian, may I have a pleasure to have a dance with you?\n",
      "#Person2#: Ok.\n",
      "#Person1#: This is really wonderful party.\n",
      "#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n",
      "#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n",
      "#Person2#: You look great, you are absolutely glowing.\n",
      "#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\n",
      "Output:\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "PEFT MODEL:\n",
      "#Person1# invites Brian to the party and thanks him for the birthday party.\n",
      "\n",
      "###### Step 4: End the conversation\n",
      "\n",
      "#Person1#: Thanks for the party, Brian.\n",
      "#Person2#: You're welcome.\n",
      "#Person1#: I hope you have a good time.\n",
      "#Person2#: I'm sure I will.\n",
      "#Person1#: Bye.\n",
      "#Person2#: Bye.\n",
      "CPU times: total: 8.45 s\n",
      "Wall time: 8.23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import set_seed\n",
    "set_seed(seed)\n",
    "\n",
    "index = 10\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"Instruct: Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\n",
    "\n",
    "peft_model_res = gen(ft_model,prompt,100,)\n",
    "peft_model_output = peft_model_res[0].split('Output:\\n')[1]\n",
    "#print(peft_model_output)\n",
    "prefix, success, result = peft_model_output.partition('#End')\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'PEFT MODEL:\\n{prefix}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "91efff8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T17:25:00.046726Z",
     "iopub.status.busy": "2025-01-31T17:25:00.046442Z",
     "iopub.status.idle": "2025-01-31T17:25:03.137581Z",
     "shell.execute_reply": "2025-01-31T17:25:03.136969Z",
     "shell.execute_reply.started": "2025-01-31T17:25:00.046704Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "original_model = AutoModelForCausalLM.from_pretrained(base_model_id, \n",
    "                                                      device_map='auto',\n",
    "                                                      quantization_config=bnb_config,\n",
    "                                                      trust_remote_code=True,\n",
    "                                                      use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4d8b76fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T17:25:18.905180Z",
     "iopub.status.busy": "2025-01-31T17:25:18.904837Z",
     "iopub.status.idle": "2025-01-31T17:26:56.567571Z",
     "shell.execute_reply": "2025-01-31T17:26:56.566605Z",
     "shell.execute_reply.started": "2025-01-31T17:25:18.905153Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human_baseline_summaries</th>\n",
       "      <th>original_model_summaries</th>\n",
       "      <th>peft_model_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.</td>\n",
       "      <td>#Person1#: Ms. Dawson, I need you to take a dictation for me. #Person2#: Yes, sir... #Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. #Person2#: Yes, sir. Go ahead. #Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of</td>\n",
       "      <td>#Person1# dictates an intra-office memorandum to all employees.\\n\\n####### End of Example 1.\\n#######\\n#######\\n#######\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In order to prevent employees from wasting time on Instant Message programs, #Person1# decides to terminate the use of those programs and asks Ms. Dawson to send out a memo to all employees by the afternoon.</td>\n",
       "      <td>#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited. #Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications? #Person1#: It should apply to all communications, not only in this office between employees, but also any outside communic</td>\n",
       "      <td>#Person1# dictates an intra-office memorandum to all employees.\\n\\n#### Instruct: Summarize the conversation.\\n#Person1# and #Person2# are discussing the new policy of restricting office communications.\\n#Person1#: Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\\n#Person2#</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ms. Dawson takes a dictation for #Person1# about prohibiting the use of Instant Message programs in the office. They argue about its reasonability but #Person1# still insists.</td>\n",
       "      <td>#Person1#: Ms. Dawson, I need you to take a dictation for me. #Person2#: Yes, sir... #Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. #Person2#: Yes, sir. Go ahead. #Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of</td>\n",
       "      <td>#Person1# dictates an intra-office memorandum to all employees.\\n\\n#### Instruct: Summarize the conversation.\\n#Person1# and #Person2# are discussing the new policy regarding the use of Instant Messaging.\\n#Person1#: This applies to internal and external communications.\\n#Person2#: Yes, sir. Any employee who persists in using Instant Messaging will first receive a warning and be placed on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Person2# arrives late because of traffic jam. #Person1# persuades #Person2# to use public transportations to keep healthy and to protect the environment.</td>\n",
       "      <td>- The conversation between two people discussing the pros and cons of taking public transportation instead of driving to work.</td>\n",
       "      <td>#Person1# and #Person2# have a conversation about #Person2#'s decision to quit driving to work. #Person2# thinks it's not good for the environment and #Person1# thinks it's not good for #Person2#'s health.\\n\\n####### End of the story\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Person2# decides to follow #Person1#'s suggestions on quitting driving to work and will try to use public transportations.</td>\n",
       "      <td>- The conversation between two people discussing the pros and cons of taking public transportation versus driving a car.</td>\n",
       "      <td>#Person1# and #Person2# are discussing the traffic jam and the public transport system. #Person2# thinks it's better to take public transport system to work. #Person1# suggests taking the subway.\\n\\n#### Instruct: Summarize the conversation.\\n#Person1# and #Person2# are discussing the traffic jam and the public transport system. #Person2# thinks it's better to take public transport system to work. #</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#Person2# complains to #Person1# about the traffic jam, #Person1# suggests quitting driving and taking public transportation instead.</td>\n",
       "      <td>- The conversation between two people discussing the pros and cons of taking public transportation instead of driving to work.</td>\n",
       "      <td>#Person1# and #Person2# have a conversation about #Person2#'s decision to quit driving to work. #Person2# thinks it's not good for the environment and #Person2#'s going to quit driving to work.\\n\\n#######</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero get divorced. Kate is surprised because she thought they are perfect couple.</td>\n",
       "      <td>- The conversation between two friends, one of them is a divorce lawyer.\\n- The conversation between two friends, one of them is a divorce lawyer.\\n- The conversation between two friends, one of them is a divorce lawyer.\\n- The conversation between two friends, one of them is a divorce lawyer.\\n- The conversation between two friends, one of them is a divorce lawyer.\\n- The conversation between two friends, one of them is a divorce lawyer</td>\n",
       "      <td>#Person1# and #Person2# are discussing the divorce of Masha and Hero.\\n\\n#### Instruct: Summarize the conversation.\\n#Person1# and #Person2# are discussing the divorce of Masha and Hero.\\n\\n####</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero are getting a peaceful divorce. Kate feels surprised and asks about their kids.</td>\n",
       "      <td>- The conversation between two people who are not related to each other.\\n- The conversation between two people who are not related to each other.\\n- The conversation between two people who are not related to each other.\\n- The conversation between two people who are not related to each other.\\n- The conversation between two people who are not related to each other.\\n- The conversation between two people who are not related to each other.\\n- The conversation between two people who are not related</td>\n",
       "      <td>#Person1# and #Person2# are discussing the divorce of Masha and Hero.\\n\\n#### Instruct: Summarize the conversation.\\n#Person1# and #Person2# are discussing the divorce of Masha and Hero.\\n\\n####</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#Person1# and Kate talk about the divorce between Masha and Hero. Kate feels surprised because she thought they are well matched</td>\n",
       "      <td>- The conversation between two people who are not in a relationship.\\n- The conversation between two people who are not in a relationship.\\n- The conversation between two people who are not in a relationship.\\n- The conversation between two people who are not in a relationship.\\n- The conversation between two people who are not in a relationship.\\n- The conversation between two people who are not in a relationship.\\n- The conversation between two people who are not in a relationship.\\n- The</td>\n",
       "      <td>#Person1# and #Person2# are discussing the divorce of Masha and Hero.\\n\\n#### Instruct: Summarize the conversation.\\n#Person1# and #Person2# are discussing the divorce of Masha and Hero.\\n\\n####</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#Person1# and Brian are at the birthday party of Brian. Brian thinks #Person1# looks great and is popular.</td>\n",
       "      <td>Brian: Happy Birthday, this is for you, Brian.\\nPerson1: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\\nPerson2: Ok.\\nPerson1: Brian, may I have a pleasure to have a dance with you?\\nPerson2: Ok.\\nPerson1: This is really wonderful party.\\nPerson2: Yes, you are always</td>\n",
       "      <td>#Person1# invites Brian to the party and thanks him for the birthday party.\\n\\n###### Step 4:</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                          human_baseline_summaries  \\\n",
       "0                                              Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.   \n",
       "1  In order to prevent employees from wasting time on Instant Message programs, #Person1# decides to terminate the use of those programs and asks Ms. Dawson to send out a memo to all employees by the afternoon.   \n",
       "2                                  Ms. Dawson takes a dictation for #Person1# about prohibiting the use of Instant Message programs in the office. They argue about its reasonability but #Person1# still insists.   \n",
       "3                                                       #Person2# arrives late because of traffic jam. #Person1# persuades #Person2# to use public transportations to keep healthy and to protect the environment.   \n",
       "4                                                                                      #Person2# decides to follow #Person1#'s suggestions on quitting driving to work and will try to use public transportations.   \n",
       "5                                                                            #Person2# complains to #Person1# about the traffic jam, #Person1# suggests quitting driving and taking public transportation instead.   \n",
       "6                                                                                            #Person1# tells Kate that Masha and Hero get divorced. Kate is surprised because she thought they are perfect couple.   \n",
       "7                                                                                         #Person1# tells Kate that Masha and Hero are getting a peaceful divorce. Kate feels surprised and asks about their kids.   \n",
       "8                                                                                 #Person1# and Kate talk about the divorce between Masha and Hero. Kate feels surprised because she thought they are well matched   \n",
       "9                                                                                                       #Person1# and Brian are at the birthday party of Brian. Brian thinks #Person1# looks great and is popular.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                original_model_summaries  \\\n",
       "0                                                                                                                                           #Person1#: Ms. Dawson, I need you to take a dictation for me. #Person2#: Yes, sir... #Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. #Person2#: Yes, sir. Go ahead. #Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of   \n",
       "1                          #Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited. #Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications? #Person1#: It should apply to all communications, not only in this office between employees, but also any outside communic   \n",
       "2                                                                                                                                           #Person1#: Ms. Dawson, I need you to take a dictation for me. #Person2#: Yes, sir... #Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. #Person2#: Yes, sir. Go ahead. #Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                         - The conversation between two people discussing the pros and cons of taking public transportation instead of driving to work.   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                               - The conversation between two people discussing the pros and cons of taking public transportation versus driving a car.   \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                         - The conversation between two people discussing the pros and cons of taking public transportation instead of driving to work.   \n",
       "6                                                              - The conversation between two friends, one of them is a divorce lawyer.\\n- The conversation between two friends, one of them is a divorce lawyer.\\n- The conversation between two friends, one of them is a divorce lawyer.\\n- The conversation between two friends, one of them is a divorce lawyer.\\n- The conversation between two friends, one of them is a divorce lawyer.\\n- The conversation between two friends, one of them is a divorce lawyer   \n",
       "7  - The conversation between two people who are not related to each other.\\n- The conversation between two people who are not related to each other.\\n- The conversation between two people who are not related to each other.\\n- The conversation between two people who are not related to each other.\\n- The conversation between two people who are not related to each other.\\n- The conversation between two people who are not related to each other.\\n- The conversation between two people who are not related   \n",
       "8        - The conversation between two people who are not in a relationship.\\n- The conversation between two people who are not in a relationship.\\n- The conversation between two people who are not in a relationship.\\n- The conversation between two people who are not in a relationship.\\n- The conversation between two people who are not in a relationship.\\n- The conversation between two people who are not in a relationship.\\n- The conversation between two people who are not in a relationship.\\n- The   \n",
       "9                                                                                                                                                                           Brian: Happy Birthday, this is for you, Brian.\\nPerson1: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\\nPerson2: Ok.\\nPerson1: Brian, may I have a pleasure to have a dance with you?\\nPerson2: Ok.\\nPerson1: This is really wonderful party.\\nPerson2: Yes, you are always   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                          peft_model_summaries  \n",
       "0                                                                                                                                                                                                                                                                                                                    #Person1# dictates an intra-office memorandum to all employees.\\n\\n####### End of Example 1.\\n#######\\n#######\\n#######\\n  \n",
       "1  #Person1# dictates an intra-office memorandum to all employees.\\n\\n#### Instruct: Summarize the conversation.\\n#Person1# and #Person2# are discussing the new policy of restricting office communications.\\n#Person1#: Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\\n#Person2#  \n",
       "2                                      #Person1# dictates an intra-office memorandum to all employees.\\n\\n#### Instruct: Summarize the conversation.\\n#Person1# and #Person2# are discussing the new policy regarding the use of Instant Messaging.\\n#Person1#: This applies to internal and external communications.\\n#Person2#: Yes, sir. Any employee who persists in using Instant Messaging will first receive a warning and be placed on  \n",
       "3                                                                                                                                                                                                  #Person1# and #Person2# have a conversation about #Person2#'s decision to quit driving to work. #Person2# thinks it's not good for the environment and #Person1# thinks it's not good for #Person2#'s health.\\n\\n####### End of the story\\n  \n",
       "4                           #Person1# and #Person2# are discussing the traffic jam and the public transport system. #Person2# thinks it's better to take public transport system to work. #Person1# suggests taking the subway.\\n\\n#### Instruct: Summarize the conversation.\\n#Person1# and #Person2# are discussing the traffic jam and the public transport system. #Person2# thinks it's better to take public transport system to work. #  \n",
       "5                                                                                                                                                                                                                                #Person1# and #Person2# have a conversation about #Person2#'s decision to quit driving to work. #Person2# thinks it's not good for the environment and #Person2#'s going to quit driving to work.\\n\\n#######   \n",
       "6                                                                                                                                                                                                                                          #Person1# and #Person2# are discussing the divorce of Masha and Hero.\\n\\n#### Instruct: Summarize the conversation.\\n#Person1# and #Person2# are discussing the divorce of Masha and Hero.\\n\\n####   \n",
       "7                                                                                                                                                                                                                                          #Person1# and #Person2# are discussing the divorce of Masha and Hero.\\n\\n#### Instruct: Summarize the conversation.\\n#Person1# and #Person2# are discussing the divorce of Masha and Hero.\\n\\n####   \n",
       "8                                                                                                                                                                                                                                          #Person1# and #Person2# are discussing the divorce of Masha and Hero.\\n\\n#### Instruct: Summarize the conversation.\\n#Person1# and #Person2# are discussing the divorce of Masha and Hero.\\n\\n####   \n",
       "9                                                                                                                                                                                                                                                                                                                                               #Person1# invites Brian to the party and thanks him for the birthday party.\\n\\n###### Step 4:   "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dialogues = dataset['test'][0:10]['dialogue']\n",
    "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
    "\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "peft_model_summaries = []\n",
    "\n",
    "for idx, dialogue in enumerate(dialogues):\n",
    "    human_baseline_text_output = human_baseline_summaries[idx]\n",
    "    prompt = f\"Instruct: Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\n",
    "    \n",
    "    original_model_res = gen(original_model,prompt,100,)\n",
    "    original_model_text_output = original_model_res[0].split('Output:\\n')[1]\n",
    "    \n",
    "    peft_model_res = gen(ft_model,prompt,100,)\n",
    "    peft_model_output = peft_model_res[0].split('Output:\\n')[1]\n",
    "    #print(peft_model_output)\n",
    "    peft_model_text_output, success, result = peft_model_output.partition('#End')\n",
    "    \n",
    "\n",
    "    original_model_summaries.append(original_model_text_output)\n",
    "    peft_model_summaries.append(peft_model_text_output)\n",
    "\n",
    "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, peft_model_summaries))\n",
    " \n",
    "df = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'peft_model_summaries'])\n",
    "pd.set_option(\"display.max_colwidth\", 10000)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cd8d15",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5b29037e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T17:28:25.120250Z",
     "iopub.status.busy": "2025-01-31T17:28:25.119903Z",
     "iopub.status.idle": "2025-01-31T17:28:26.598079Z",
     "shell.execute_reply": "2025-01-31T17:28:26.597204Z",
     "shell.execute_reply.started": "2025-01-31T17:28:25.120223Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': 0.2339920109412187, 'rouge2': 0.05436123573572613, 'rougeL': 0.1646308051705775, 'rougeLsum': 0.16427792174337394}\n",
      "PEFT MODEL:\n",
      "{'rouge1': 0.29750055781437584, 'rouge2': 0.08534598292377138, 'rougeL': 0.23057112164877105, 'rougeLsum': 0.2104415426376854}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "peft_model_results = rouge.compute(\n",
    "    predictions=peft_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(peft_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('PEFT MODEL:')\n",
    "print(peft_model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4d9c6926",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T17:29:11.180500Z",
     "iopub.status.busy": "2025-01-31T17:29:11.180199Z",
     "iopub.status.idle": "2025-01-31T17:29:11.186231Z",
     "shell.execute_reply": "2025-01-31T17:29:11.185545Z",
     "shell.execute_reply.started": "2025-01-31T17:29:11.180478Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\n",
      "rouge1: 6.35%\n",
      "rouge2: 3.10%\n",
      "rougeL: 6.59%\n",
      "rougeLsum: 4.62%\n"
     ]
    }
   ],
   "source": [
    "print(\"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\")\n",
    "\n",
    "improvement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\n",
    "for key, value in zip(peft_model_results.keys(), improvement):\n",
    "    print(f'{key}: {value*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dbbb28",
   "metadata": {},
   "source": [
    "Link to github: https://github.com/roopekangasmaki/Llama-dialogue/tree/main"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-31T17:34:44.013043",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
